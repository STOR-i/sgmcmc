<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Advanced Example: Simulate from a Bayesian Neural Network – Storage Constraints • sgmcmc</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sgmcmc</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/sgmcmc.html">Get Started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/gaussMixture.html">Worked Example: Simulate from a Gaussian Mixture</a>
    </li>
    <li>
      <a href="../articles/logisticRegression.html">Worked Example: Logistic Regression</a>
    </li>
    <li>
      <a href="../articles/mvGauss.html">Worked Example: Simulate from a Multivariate Gaussian</a>
    </li>
    <li>
      <a href="../articles/nn.html">Advanced Example: Simulate from a Bayesian Neural Network -- Storage Constraints</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/STOR-i/sgmcmc">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Advanced Example: Simulate from a Bayesian Neural Network – Storage Constraints</h1>
                        <h4 class="author">Jack Baker</h4>
            
          </div>

    
    
<div class="contents">
<p>Often big datasets and high dimensionality go hand in hand. Sometimes the dimensionality is so high that storage of the full MCMC chain in memory becomes an issue. There are a number of ways around this, including: calculating the Monte Carlo estimate on the fly; reducing the dimensionality of the chain using a test function; or just periodically saving a the chain to the hard disk and starting from scratch. To give you more flexibility we allow an SGMCMC algorithm to be run step by step. This allows you to do what you want with the output of the chain. This guide goes into more detail about how to do this, but it needs more TensorFlow knowledge, such as knowledge of TensorFlow sessions and how to build your own placeholders. For more details on these see the <a href="https://tensorflow.rstudio.com/">TensorFlow for R documentation</a>.</p>
<p>To demonstrate this concept we fit a two layer Bayesian neural network to the MNIST dataset. The MNIST dataset consists of <span class="math inline">\(28 \times 28\)</span> pixel images of handwritten digits from zero to nine. The images are flattened to be a vector of length 784. The dataset is available as a standard dataset from the TensorFlow library, with a matrix of 55000 training vectors and 10000 test vectors, each with their corresponding labels. First, let’s construct the dataset and a testset. We assume you’ve read some of the earlier vignettes, so are familiar with how to do this. The MNIST dataset can be downloaded using the <code>sgmcmc</code> function <code>getDataset</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sgmcmc)
<span class="co"># Download and load MNIST dataset</span>
mnist =<span class="st"> </span><span class="kw"><a href="../reference/getDataset.html">getDataset</a></span>(<span class="st">"mnist"</span>)
<span class="co"># Build dataset list and testset list</span>
dataset =<span class="st"> </span><span class="kw">list</span>(<span class="st">"X"</span> =<span class="st"> </span>mnist$train$images, <span class="st">"y"</span> =<span class="st"> </span>mnist$train$labels)
testset =<span class="st"> </span><span class="kw">list</span>(<span class="st">"X"</span> =<span class="st"> </span>mnist$test$images, <span class="st">"y"</span> =<span class="st"> </span>mnist$test$labels)</code></pre></div>
<p>We’ll build the same neural network model as in the original SGHMC paper <a href="https://arxiv.org/pdf/1402.4102v2.pdf">(Chen et. al 2014)</a>. Suppose <span class="math inline">\(Y_i\)</span> takes values in <span class="math inline">\(\{0,\dots,9\}\)</span>, so is the output label of a digit, and <span class="math inline">\(\mathbf x_i\)</span> is the input vector, with <span class="math inline">\(\mathbf X\)</span> the full <span class="math inline">\(N \times 784\)</span> dataset, where <span class="math inline">\(N\)</span> is the number of observations. Then we model as follows <span class="math display">\[
    Y_i | \theta, \mathbf x_i \sim \text{Categorical}( \beta(\theta, \mathbf x_i) ), \\
    \beta(\theta, \mathbf x_i) = \sigma \left( \sigma \left( \mathbf x_i^T B + b \right) A + a \right).
\]</span> Here <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> are parameters to be inferred with <span class="math inline">\(\theta = (A, B, a, b)\)</span>; <span class="math inline">\(\sigma(.)\)</span> is the softmax function. <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are matrices with dimensions: <span class="math inline">\(100 \times 10\)</span>, <span class="math inline">\(784 \times 100\)</span>, <span class="math inline">\(1 \times 10\)</span> and <span class="math inline">\(1 \times 100\)</span> respectively. Each element of these parameters is given a Normal prior to give <span class="math display">\[
    A_{kl} | \lambda_A \sim N(0, \lambda_A^{-1}), \quad B_{jk} | \lambda_B \sim N(0, \lambda_B^{-1}), \\
    a_l | \lambda_a \sim N(0, \lambda_a^{-1}), \quad b_k | \lambda_b \sim N(0, \lambda_b^{-1}), \\
    j = 1,\dots,784; \quad k = 1,\dots,100; \quad l = 1,\dots,10;
\]</span> where <span class="math inline">\(\lambda_A\)</span>, <span class="math inline">\(\lambda_B\)</span>, <span class="math inline">\(\lambda_a\)</span> and <span class="math inline">\(\lambda_b\)</span> are hyperparameters. Finally we assume <span class="math display">\[
    \lambda_A, \lambda_B, \lambda_a, \lambda_b \sim \text{Gamma}(1, 1).
\]</span></p>
<p>As you can see this is a lot of high dimensional parameters, and unless you have a lot of RAM to hand, a standard chain of length <span class="math inline">\(10^4\)</span> will not fit into memory. First let’s create the <code>params</code> dictionary, and then we can code the <code>logLik</code> and <code>logPrior</code> functions. We’ll sample initial <span class="math inline">\(\lambda\)</span> parameters from a standard Gamma, and the rest from a standard Normal as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Sample initial weights from standard Normal</span>
d =<span class="st"> </span><span class="kw">ncol</span>(dataset$X) <span class="co"># dimension of chain</span>
params =<span class="st"> </span><span class="kw">list</span>()
params$A =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">rnorm</span>(<span class="dv">10</span>*<span class="dv">100</span>), <span class="dt">ncol =</span> <span class="dv">10</span> )
params$B =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(d*<span class="dv">100</span>), <span class="dt">ncol =</span> <span class="dv">100</span>)
<span class="co"># Sample initial bias parameters from standard Normal</span>
params$a =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span>)
params$b =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)
<span class="co"># Sample initial precision parameters from standard Gamma</span>
params$lambdaA =<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>, <span class="dv">1</span>)
params$lambdaB =<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>, <span class="dv">1</span>)
params$lambdaa =<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>, <span class="dv">1</span>)
params$lambdab =<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>, <span class="dv">1</span>)</code></pre></div>
<p>Now let’s declare the <code>logLik</code> and <code>logPrior</code> functions</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logLik =<span class="st"> </span>function(params, dataset) {
    <span class="co"># Calculate estimated probabilities</span>
    beta =<span class="st"> </span>tf$nn$<span class="kw">softmax</span>(tf$<span class="kw">matmul</span>(dataset$X, params$B) +<span class="st"> </span>params$b)
    beta =<span class="st"> </span>tf$nn$<span class="kw">softmax</span>(tf$<span class="kw">matmul</span>(beta, params$A) +<span class="st"> </span>params$a)
    <span class="co"># Calculate log likelihood of categorical distn with probabilities beta</span>
    logLik =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(dataset$y *<span class="st"> </span>tf$<span class="kw">log</span>(beta))
    <span class="kw">return</span>(logLik)
}

logPrior =<span class="st"> </span>function(params) {
    distLambda =<span class="st"> </span>tf$contrib$distributions$<span class="kw">Gamma</span>(<span class="dv">1</span>, <span class="dv">1</span>)
    distA =<span class="st"> </span>tf$contrib$distributions$<span class="kw">Normal</span>(<span class="dv">0</span>, tf$<span class="kw">rsqrt</span>(params$lambdaA))
    logPriorA =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(distA$<span class="kw">log_prob</span>(params$A)) +<span class="st"> </span>distLambda$<span class="kw">log_prob</span>(params$lambdaA)
    distB =<span class="st"> </span>tf$contrib$distributions$<span class="kw">Normal</span>(<span class="dv">0</span>, tf$<span class="kw">rsqrt</span>(params$lambdaB))
    logPriorB =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(distB$<span class="kw">log_prob</span>(params$B)) +<span class="st"> </span>distLambda$<span class="kw">log_prob</span>(params$lambdaB)
    dista =<span class="st"> </span>tf$contrib$distributions$<span class="kw">Normal</span>(<span class="dv">0</span>, tf$<span class="kw">rsqrt</span>(params$lambdaa))
    logPriora =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(dista$<span class="kw">log_prob</span>(params$a)) +<span class="st"> </span>distLambda$<span class="kw">log_prob</span>(params$lambdaa)
    distb =<span class="st"> </span>tf$contrib$distributions$<span class="kw">Normal</span>(<span class="dv">0</span>, tf$<span class="kw">rsqrt</span>(params$lambdab))
    logPriorb =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(distb$<span class="kw">log_prob</span>(params$b)) +<span class="st"> </span>distLambda$<span class="kw">log_prob</span>(params$lambdab)
    logPrior =<span class="st"> </span>logPriorA +<span class="st"> </span>logPriorB +<span class="st"> </span>logPriora +<span class="st"> </span>logPriorb
    <span class="kw">return</span>(logPrior)
}</code></pre></div>
<p>Now suppose we want to make inference using stochastic gradient Langevin dynamics (SGLD). If we do this in the normal way then we will most likely run out of memory when the function builds the array to store the output. So instead we just initialize an <code>sgld</code> object using <code>sgldSetup</code>. Similarly we could build an <code>sgldcv</code> object using <code>sgldcvSetup</code> or an <code>sgnht</code> object using <code>sgnhtSetup</code>. Then we can run the SGLD algorithm one step at a time and decide what to do with the output at each iteration ourselves. We’ll just set our stepsize to <code>1e-4</code> for this example. To make the results reproducible we’ll set the seed to 13.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stepsize =<span class="st"> </span><span class="fl">1e-4</span>
sgld =<span class="st"> </span><span class="kw"><a href="../reference/sgldSetup.html">sgldSetup</a></span>(logLik, dataset, params, stepsize, <span class="dt">logPrior =</span> logPrior, 
        <span class="dt">minibatchSize =</span> <span class="dv">500</span>, <span class="dt">seed =</span> <span class="dv">13</span>)</code></pre></div>
<p>This <code>sgld</code> object is a type of <code>sgmcmc</code> object, which is basically just a list with a number of entries. The most important of these entries to us is called <code>params</code>, which holds a list, with the same names as you had in the <code>params</code> you fed to <code>sgld</code>, but this list contains <code>tf$Variable</code> objects. This is how you access the tensors which hold your current parameter values in the chain. For more details on the attributes of these objects, see the documentation for <code>sgldSetup</code>, <code>sgldcvSetup</code> etc.</p>
<p>Now that we have created the <code>sgld</code> object, you want to initialise the TensorFlow graph and the <code>sgmcmc</code> algorithm you’ve chosen. If you are using a standard algorthm, this will just initialise the TensorFlow graph and all the tensors that were created. If you’re using an algorithm with control variates (e.g. <code>sgldcv</code>), then this will also find the MAP estimates of the parameter and calculate the full log posterior gradient at that point. The function we use to do this is <code>initSess</code> as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sess =<span class="st"> </span><span class="kw"><a href="../reference/initSess.html">initSess</a></span>(sgld)</code></pre></div>
<p>The <code>sess</code> returned by <code>initSess</code> is the current TensorFlow session, which is needed to run the SGMCMC algorithm of choice, and to access any of the tensors you need, such as <code>sgld$params</code>.</p>
<p>Now we have everything to run an SGLD algorithm step by step as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">for (i in <span class="dv">1</span>:<span class="dv">10</span>^3L) {
    <span class="kw"><a href="../reference/sgmcmcStep.html">sgmcmcStep</a></span>(sgld, sess)
    currentState =<span class="st"> </span><span class="kw"><a href="../reference/getParams.html">getParams</a></span>(sgld, sess)
}</code></pre></div>
<p>Here the function <code>sgmcmcStep</code> will update <code>sgld$params</code> using a single update of SGLD, or whichever SGMCMC algorithm you chose. The function <code>getParams</code> will return a list of parameters as <code>R</code> objects rather than as tensors to make life easier for you.</p>
<p>Our simple example is fine, but we really would like to calculate a Monte Carlo average of the parameters on the fly. Also with these large examples, they take a long time to run, so it’s useful to check how the algorithm is doing every once in a while. This is especially useful when tuning by trial and error as you can stop an algorithm early if it’s doing badly. This is why we let you declare the TensorFlow session yourself: it lets you create your custom tensors to print algorithm progress, or to create your own test functions to reduce the chain dimensionality (they have to be declared before the TensorFlow session starts).</p>
<p>Let’s delete everything after we created our <code>sgld</code> object. Now we’re going to demonstrate a more complicated step by step example where we print performance and calulate the Monte Carlo estimate on the fly. Suppose we have test data <span class="math inline">\(X^*\)</span>, and test labels <span class="math inline">\(y^*\)</span>, and at some iteration <span class="math inline">\(i\)</span> our SGMCMC algorithm outputs values for all the parameters <span class="math inline">\(\theta_t\)</span>. Then the probability that our neural network model will classify a given test observation to class <span class="math inline">\(k\)</span> is given by <span class="math inline">\(\beta_k(\theta_t, \mathbf x_i^*)\)</span>; i.e. the <span class="math inline">\(k^{th}\)</span> element of <span class="math inline">\(\beta(\theta_t, \mathbf x_i^*)\)</span>, which was defined earlier. A common performance measure for a classifier is the <a href="https://www.kaggle.com/wiki/LogLoss">log loss</a>, defined by <span class="math display">\[
    ll = - \frac{1}{N} \sum_{i=1}^{N_{\text{test}}} \sum_{k=1}^K y^*_{i,k} \log \beta( \theta_t, \mathbf x_i^* ).
\]</span></p>
<p>This is also <span class="math inline">\(-\frac{1}{N}\)</span> times the log likelihood at the current parameter, given the test set. So it’s very easy for us to calculate this in practice and output it. We’ll do this every 100 iterations to check the algorithm’s performance and check for convergence. To do this, we need to create a new placeholder to hold the test set, and then create a tensor that will calculate the log loss, which can easily be done using the <code>logLik</code> function already declared. First we’ll create a placeholder for both <code>X</code> and <code>y</code> in the test set, and make sure these have the same dimensions so can hold the full test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">testPlaceholder =<span class="st"> </span><span class="kw">list</span>()
testPlaceholder[[<span class="st">"X"</span>]] =<span class="st"> </span>tf$<span class="kw">placeholder</span>(tf$float32, <span class="kw">dim</span>(testset[[<span class="st">"X"</span>]]))
testPlaceholder[[<span class="st">"y"</span>]] =<span class="st"> </span>tf$<span class="kw">placeholder</span>(tf$float32, <span class="kw">dim</span>(testset[[<span class="st">"y"</span>]]))</code></pre></div>
<p>Now we can create a tensor that calculates the log loss. We’ll link this to our <code>testPlaceholder</code> and the current parameter values, located at <code>sgld$params</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get number of observations in test set, ensuring it's a double (R equivalent of float)</span>
Ntest =<span class="st"> </span><span class="kw">as.double</span>(<span class="kw">nrow</span>(testset[[<span class="st">"X"</span>]]))
logLoss =<span class="st"> </span>-<span class="st"> </span><span class="kw">logLik</span>(sgld$params, testPlaceholder) /<span class="st"> </span>Ntest</code></pre></div>
<p>Now we’ll declare the TensorFlow session, and run the chain step by step, calculating an average parameter estimate and printing the log loss of the current state every 100 iterations</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sess =<span class="st"> </span><span class="kw"><a href="../reference/initSess.html">initSess</a></span>(sgld)
<span class="co"># Fill a feed dict with full test set (used to calculate log loss)</span>
feedDict =<span class="st"> </span><span class="kw">dict</span>()
feedDict[[testPlaceholder[[<span class="st">"X"</span>]]]] =<span class="st"> </span>testset[[<span class="st">"X"</span>]]
feedDict[[testPlaceholder[[<span class="st">"y"</span>]]]] =<span class="st"> </span>testset[[<span class="st">"y"</span>]]
<span class="co"># Burn-in chain</span>
<span class="kw">message</span>(<span class="st">"Burning-in chain..."</span>)
<span class="kw">message</span>(<span class="st">"iteration</span><span class="ch">\t</span><span class="st">log loss"</span>)
for (i in <span class="dv">1</span>:<span class="dv">10</span>^<span class="dv">3</span>) {
    <span class="co"># Print progress</span>
    if (i %%<span class="st"> </span><span class="dv">100</span> ==<span class="st"> </span><span class="dv">0</span>) {
        progress =<span class="st"> </span>sess$<span class="kw">run</span>(logLoss, <span class="dt">feed_dict =</span> feedDict)
        <span class="kw">message</span>(<span class="kw">paste0</span>(i, <span class="st">"</span><span class="ch">\t</span><span class="st">"</span>, progress))
    }
    <span class="kw"><a href="../reference/sgmcmcStep.html">sgmcmcStep</a></span>(sgld, sess)
}
<span class="co"># Initialise Monte Carlo estimate using value after burn-in</span>
avParams =<span class="st"> </span><span class="kw"><a href="../reference/getParams.html">getParams</a></span>(sgld, sess)
<span class="co"># Run chain</span>
<span class="kw">message</span>(<span class="st">"Running SGMCMC..."</span>)
for (i in <span class="dv">1</span>:<span class="dv">10</span>^<span class="dv">4</span>) {
    <span class="kw"><a href="../reference/sgmcmcStep.html">sgmcmcStep</a></span>(sgld, sess)
    <span class="co"># Update av Params</span>
    currentState =<span class="st"> </span><span class="kw"><a href="../reference/getParams.html">getParams</a></span>(sgld, sess)
    for (paramName in <span class="kw">names</span>(avParams)) {
        avParams[[paramName]] =<span class="st"> </span>(avParams[[paramName]] *<span class="st"> </span>i +<span class="st"> </span>currentState[[paramName]]) /<span class="st"> </span>(i +<span class="st"> </span><span class="dv">1</span>)
    }
    <span class="co"># Print progress</span>
    if (i %%<span class="st"> </span><span class="dv">100</span> ==<span class="st"> </span><span class="dv">0</span>) {
        progress =<span class="st"> </span>sess$<span class="kw">run</span>(logLoss, <span class="dt">feed_dict =</span> feedDict)
        <span class="kw">message</span>(<span class="kw">paste0</span>(i, <span class="st">"</span><span class="ch">\t</span><span class="st">"</span>, progress))
    }
}</code></pre></div>
<p>Obviously calculating the log loss is costly to do every 100 iterations as the test set has <span class="math inline">\(10^4\)</span> observations itself, this was just for demonstration purposes. In practice we’d recommend subsampling this test set when calculating the log loss, or leaving more iterations until it’s calculated.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="http://lancs.ac.uk/~bakerj1/">Jack Baker</a>, <a href="http://www.lancs.ac.uk/~nemeth/">Christopher Nemeth</a>, <a href="http://www.maths.lancs.ac.uk/~fearnhea/">Paul Fearnhead</a>, <a href="https://homes.cs.washington.edu/~ebfox/">Emily B. Fox</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
