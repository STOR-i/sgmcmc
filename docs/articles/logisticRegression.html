<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Worked Example: Logistic Regression • sgmcmc</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sgmcmc</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/sgmcmc.html">Get Started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/gaussMixture.html">Worked Example: Simulate from a Gaussian Mixture</a>
    </li>
    <li>
      <a href="../articles/logisticRegression.html">Worked Example: Logistic Regression</a>
    </li>
    <li>
      <a href="../articles/mvGauss.html">Worked Example: Simulate from a Multivariate Gaussian</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Worked Example: Logistic Regression</h1>
                        <h4 class="author">Jack Baker</h4>
            
          </div>

    
    
<div class="contents">
<p>In this example we use the package to infer the bias and coefficients in a logistic regression model using <a href="https://arxiv.org/pdf/1706.05439.pdf">stochastic gradient Langevin Dynamics with control variates</a>. So we assume we have data <span class="math inline">\(\mathbf x_1, \dots, \mathbf x_N\)</span> and response variables <span class="math inline">\(y_1, \dots, y_N\)</span> with likelihood <span class="math display">\[
    p(\mathbf X, \mathbf y | \beta, \beta_0 ) = \prod_{i=1}^N \left[ \frac{1}{1+e^{-\beta_0 + \mathbf x_i \beta}} \right]^{y_i} \left[ 1 - \frac{1}{1+e^{-\beta_0 + \mathbf x_i \beta}} \right]^{1-y_i}
\]</span></p>
<p>First let’s load in the data, we will use the <a href="https://archive.ics.uci.edu/ml/datasets/covertype">cover type dataset</a> commonly used to benchmark classification models. We use the <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html">dataset from LIBSVM</a>, which transforms the problem from multiclass to binary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sgmcmc)
<span class="co"># Load data from package</span>
<span class="kw">data</span>(<span class="st">"covertype"</span>)</code></pre></div>
<p>First we’ll remove about 10000 observations from the original dataset, this will be used to check that our algorithm did okay. Then we’ll separate out the response variable <code>y</code> and the explanatory variables <code>X</code>. The response variable is the first column in the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">testObservations =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(covertype), <span class="dv">10</span>^<span class="dv">4</span>)
testSet =<span class="st"> </span>covertype[testObservations,]
X =<span class="st"> </span>covertype[-<span class="kw">c</span>(testObservations),<span class="dv">2</span>:<span class="kw">ncol</span>(covertype)]
y =<span class="st"> </span>covertype[-<span class="kw">c</span>(testObservations),<span class="dv">1</span>]
dataset =<span class="st"> </span><span class="kw">list</span>( <span class="st">"X"</span> =<span class="st"> </span>X, <span class="st">"y"</span> =<span class="st"> </span>y )</code></pre></div>
<p>In the last line we defined the dataset as it will be input to the relevant <code>sgmcmc</code> function. A lot of the inputs to functions in <code>sgmcmc</code> are defined as lists. This improves flexibility by enabling models to be specified with multiple parameters, datasets and allows separate tuning constants to be set for each parameter. We assume that observations are always accessed on the first dimension of each object, i.e. the point <span class="math inline">\(x_i\)</span> is located at <code>X[i,]</code> rather than <code>X[,i]</code>. Similarly the observation <span class="math inline">\(i\)</span> from a 3d object <code>Y</code> would be located at <code>Y[i,,]</code>.</p>
<p>Now we want to set the starting values and shapes for our parameters. We can see from the likelihood equation we have two parameters, the bias <span class="math inline">\(\beta_0\)</span> and the coefficients <span class="math inline">\(\beta\)</span>. We’ll just set these to start from zero. Similar to the data, these are just a list with the relevant names.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get the dimension of X, needed to set shape of params$beta</span>
d =<span class="st"> </span><span class="kw">ncol</span>(dataset$X)
params =<span class="st"> </span><span class="kw">list</span>( <span class="st">"bias"</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">"beta"</span> =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">rep</span>( <span class="dv">0</span>, d ), <span class="dt">nrow =</span> d ) )</code></pre></div>
<p>Now we’ll define the functions <code>logLik</code> and <code>logPrior</code>. It should now become clear why the list names come in handy. The function <code>logLik</code> should take two parameters as input: <code>params</code> and <code>dataset</code>. These parameters will be lists with the same names as those you defined for <code>params</code> and <code>dataset</code> earlier. There is one difference though, the objects in the lists will have automatically been converted to <code>TensorFlow</code> objects for you. The <code>params</code> list will contain <code>TensorFlow</code> tensor variables; the <code>dataset</code> list will contain <code>TensorFlow</code> placeholders. The <code>logLik</code> function should take these lists as input and return the value of the log likelihood as a tensor at point <code>params</code> given data <code>dataset</code>. The function should do this using <code>TensorFlow</code> operations, as this allows the gradient to be automatically calculated; it also allows the wide range of distribution objects as well as matrix operations that <code>TensorFlow</code> provides to be taken advantage of. A tutorial of <code>TensorFlow</code> for <code>R</code> is beyond the scope of this article, for more details we refer the reader to the website of <a href="https://tensorflow.rstudio.com/">TensorFlow for R</a>. With this in place we can define the <code>logLik</code> function as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logLik =<span class="st"> </span>function(params, dataset) {
    yEstimated =<span class="st"> </span><span class="dv">1</span> /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>tf$<span class="kw">exp</span>( -<span class="st"> </span>tf$<span class="kw">squeeze</span>(params$bias +<span class="st"> </span>tf$<span class="kw">matmul</span>(dataset$X, params$beta))))
    logLik =<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(dataset$y *<span class="st"> </span>tf$<span class="kw">log</span>(yEstimated) +<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>dataset$y) *<span class="st"> </span>tf$<span class="kw">log</span>(<span class="dv">1</span> -<span class="st"> </span>yEstimated))
    <span class="kw">return</span>(logLik)
}</code></pre></div>
<p><code>R</code> does not play nicely with the tensorflow type system, so make sure you set all your constants in the <code>logLik</code> and <code>logPrior</code> functions to <code>tf$float32</code>. Otherwise you will encounter an error.</p>
<p>Next we want to define our log prior, we assume each <span class="math inline">\(\beta_i\)</span> have independent prior Laplace distribution, so that <span class="math inline">\(\log p( \beta ) = - \sum_{i=0}^d | \beta_i|\)</span>. Similar to the log likelihood definition, the log prior is defined as a function with input <code>params</code>. In our case the definition is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logPrior =<span class="st"> </span>function(params) {
    logPrior =<span class="st"> </span>-<span class="st"> </span>(tf$<span class="kw">reduce_sum</span>(tf$<span class="kw">abs</span>(params$beta)) +<span class="st"> </span>tf$<span class="kw">reduce_sum</span>(tf$<span class="kw">abs</span>(params$bias)))
    <span class="kw">return</span>(logPrior)
}</code></pre></div>
<p>Again make sure you set all your constants inside the function to <code>tf$float32</code>.</p>
<p>Finally we’ll set the stepsize parameters for the algorithm, along with the minibatch size. <code>sgldcv</code> relies on two stepsize parameters, one for the optimization step and one for the MCMC step. The values of the optimization and MCMC stepsizes will generally be quite similar. To allow stepsizes to be set for different parameters, the form of the stepsizes for the MCMC will be lists with names corresponding to each of the names in <code>params</code>. The optimization step will just be one value as the stepsize is automatically tuned</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stepsizesMCMC =<span class="st"> </span><span class="kw">list</span>(<span class="st">"beta"</span> =<span class="st"> </span><span class="fl">2e-5</span>, <span class="st">"bias"</span> =<span class="st"> </span><span class="fl">2e-5</span>)
stepsizesOptimization =<span class="st"> </span><span class="fl">1e-1</span>
<span class="co"># Set minibatch size</span>
n =<span class="st"> </span><span class="dv">500</span></code></pre></div>
<p>Now we can run our SGLD-CV algorithm using the function <code>sgldcv</code> from the <code>sgmcmc</code> package, which returns a list of Markov chains for each parameter as output. Use the argument <code>verbose = FALSE</code> to hide the output of the function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">output =<span class="st"> </span><span class="kw"><a href="../reference/sgldcv.html">sgldcv</a></span>(logLik, logPrior, dataset, params, stepsizesMCMC, stepsizesOptimization, n, <span class="dt">nIters =</span> <span class="fl">1.1e4</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>To check the algorithm converged, we’ll plot the average log predictive density of the data from our test set every 10 iterations. Let <span class="math display">\[\hat \pi_i^{(j)} := \frac{1}{1 + \exp\left[-\beta_0^{(j)} - \mathbb x_i \beta^{(j)}\right]},\]</span> here <span class="math inline">\(\hat \pi_i^{(j)}\)</span> denotes the probability that the <span class="math inline">\(j^{\text{th}}\)</span> iteration of our MCMC chain assigned to observation <span class="math inline">\(i\)</span> in our test set. Define our test set by <span class="math inline">\(T\)</span>, the average log predictive is given by <span class="math display">\[A := \frac{1}{|T|} \sum_{y_i \in T} \left[ y_i \log \hat \pi_i^{(j)} + (1 - y_i) \log(1 - \hat \pi_i^{(j)}) \right]\]</span></p>
<p>To check convergence, we’ll plot the average log predictive every 10 iterations as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yTest =<span class="st"> </span>testSet[,<span class="dv">1</span>]
XTest =<span class="st"> </span>testSet[,<span class="dv">2</span>:<span class="kw">ncol</span>(testSet)]
<span class="co"># Remove burn-in</span>
output$bias =<span class="st"> </span>output$bias[-<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">1000</span>)]
output$beta =<span class="st"> </span>output$beta[-<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">1000</span>),,]
iterations =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">10</span>^<span class="dv">4</span>, <span class="dt">by =</span> <span class="dv">10</span>)
avLogPred =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(iterations))
<span class="co"># Calculate log predictive every 10 iterations</span>
for ( iter in <span class="dv">1</span>:<span class="kw">length</span>(iterations) ) {
    j =<span class="st"> </span>iterations[iter]
    <span class="co"># Get parameters at iteration j</span>
    beta0_j =<span class="st"> </span>output$bias[j]
    beta_j =<span class="st"> </span>output$beta[j,]
    for ( i in <span class="dv">1</span>:<span class="kw">length</span>(yTest) ) {
        pihat_ij =<span class="st"> </span><span class="dv">1</span> /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-<span class="st"> </span>beta0_j -<span class="st"> </span><span class="kw">sum</span>(XTest[i,] *<span class="st"> </span>beta_j)))
        y_i =<span class="st"> </span>yTest[i]
        <span class="co"># Calculate log predictive at current test set point</span>
        LogPred_curr =<span class="st"> </span>-<span class="st"> </span>(y_i *<span class="st"> </span><span class="kw">log</span>(pihat_ij) +<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>y_i) *<span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> -<span class="st"> </span>pihat_ij))
        avLogPred[iter] =<span class="st"> </span>avLogPred[iter] +<span class="st"> </span><span class="dv">1</span> /<span class="st"> </span><span class="kw">length</span>(yTest) *<span class="st"> </span>LogPred_curr
    }
}
<span class="kw">library</span>(ggplot2)
plotFrame =<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">"iteration"</span> =<span class="st"> </span>iterations, <span class="st">"logPredictive"</span> =<span class="st"> </span>avLogPred)
<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(plotFrame, <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(<span class="dt">x =</span> iteration, <span class="dt">y =</span> logPredictive)) +
<span class="st">    </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_path">geom_line</a></span>() +
<span class="st">    </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/labs">xlab</a></span>(<span class="st">"Average log predictive of test set"</span>)</code></pre></div>
<p><img src="logisticRegression_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="http://lancs.ac.uk/~bakerj1/">Jack Baker</a>, <a href="http://www.lancs.ac.uk/~nemeth/">Christopher Nemeth</a>, <a href="http://www.maths.lancs.ac.uk/~fearnhea/">Paul Fearnhead</a>, <a href="http://www.stor-i.lancs.ac.uk/"><img src="http://www.stor-i.lancs.ac.uk/MediaGallery/9-144.jpg" height="18"></a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
