% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sgld.R
\name{sgldcv}
\alias{sgldcv}
\title{Stochastic Gradient Langevin Dynamics with Control Variates}
\usage{
sgldcv(logLik, dataset, params, stepsize, optStepsize, logPrior = NULL,
  minibatchSize = 0.01, nIters = 10^4L, nItersOpt = 10^4L,
  verbose = TRUE)
}
\arguments{
\item{logLik}{function which takes parameters and dataset 
(list of TensorFlow variables and placeholders respectively) as input. 
It should return a TensorFlow expression which defines the log likelihood of the model.}

\item{dataset}{list of numeric R arrays which defines the datasets for the problem.
The names in the list should correspond to those referred to in the logLik and logPrior functions}

\item{params}{list of numeric R arrays which define the starting point of each parameter.
The names in the list should correspond to those referred to in the logLik and logPrior functions}

\item{stepsize}{list of numeric values corresponding to the SGLD stepsizes for each parameter
The names in the list should correspond to those in params.
Alternatively specify a single numeric value to use that stepsize for all parameters.}

\item{optStepsize}{numeric value specifying the stepsize for the optimization 
to find MAP estimates of parameters. The TensorFlow AdamOptimizer is used.}

\item{logPrior}{optional. Default uninformative improper prior.
Function which takes parameters (list of TensorFlow variables) as input.
The function should return a TensorFlow tensor which defines the log prior of the model.}

\item{minibatchSize}{optional. Default 0.01.
Numeric or integer value that specifies amount of dataset to use at each iteration 
either as proportion of dataset size (if between 0 and 1) or actual magnitude (if an integer).}

\item{nIters}{optional. Default 10^4L. Integer specifying number of iterations to perform.}

\item{nItersOpt}{optional. Default 10^4L. 
Integer specifying number of iterations of initial optimization to perform.}

\item{verbose}{optional. Default TRUE. Boolean specifying whether to print algorithm progress}
}
\value{
Returns list of arrays for each parameter containing the MCMC chain.
 Dimension of the form (nIters,paramDim1,paramDim2,...)
}
\description{
Simulates from the posterior defined by the functions logLik and logPrior using
 stochastic gradient Langevin Dynamics with an improved gradient estimate using Control Variates.
 The function uses TensorFlow, so needs Tensorflow for python installed.
}
\examples{
# Simulate from a Normal Distribution with uninformative prior
dataset = list("x" = rnorm(1000))
params = list("theta" = 0)
logLik = function(params, dataset) { 
    distn = tf$contrib$distributions$Normal(params$theta, 1)
    return(tf$reduce_sum(distn$log_prob(dataset$x)))
}
stepsize = list("theta" = 1e-4)
optStepsize = 1e-1
output = sgldcv(logLik, dataset, params, stepsize, optStepsize)
}
\references{
\itemize{
 \item \href{https://arxiv.org/pdf/1706.05439.pdf}{
 Baker, J., Fearnhead, P., Fox, E. B., and Nemeth, C. (2017).
 Control variates for stochastic gradient MCMC. ArXiv preprint arXiv:1706.05439.}
 \item \href{http://people.ee.duke.edu/~lcarin/398_icmlpaper.pdf}{
 Welling, M., and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. 
 ICML (pp. 681-688).} }
}
