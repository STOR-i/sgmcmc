---
title: "Worked Example: Simulate from a Gaussian Mixture"
author: "Jack Baker"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this example we use the package to infer the modes of a bimodal, 2d Gaussian using [stochastic gradient Hamiltonian Monte Carlo](https://arxiv.org/pdf/1402.4102v2.pdf). So we assume we have iid data $x_1, \dots, x_N$ with $x_i | \theta \sim 0.5 N( \theta_1, I_2 ) + 0.5 N( \theta_2, I_2 )$, and we want to infer $\theta_1$ and $\theta_2$.

First let's simulate the data with the following code, we set $N$ to be $10^4$
```{r message=FALSE}
library(sgmcmc)
library(MASS)
# Declare number of observations
N = 10^4
# Set locations of two modes, theta1 and theta2
theta1 = c( 0, 0 )
theta2 = c( 0.1, 0.1 )
# Allocate observations to each component
z = sample( 2, N, replace = TRUE, prob = c( 0.5, 0.5 ) )
# Predeclare data matrix
X = matrix( rep( NA, 2*N ), ncol = 2 )
# Simulate each observation depending on the component its been allocated
for ( i in 1:N ) {
    if ( z[i] == 1 ) {
        X[i,] = mvrnorm( 1, theta1, diag(2) )
    } else {
        X[i,] = mvrnorm( 1, theta2, diag(2) )
    }
}
dataset = list("X" = X)
```
In the last line we defined the dataset as it will be input to the relevant `sgmcmc` function. A lot of the inputs to functions in `sgmcmc` are defined as lists. This improves flexibility by enabling models to be specified with multiple parameters, datasets and allows separate tuning constants to be set for each parameter. We assume that observations are always accessed on the first dimension of each object, i.e. the point $x_i$ is located at `X[i,]` rather than `X[,i]`. Similarly the observation $i$ from a 3d object `Y` would be located at `Y[i,,]`.

The parameters are declared very similarly, but this time the value associated with each entry is its starting point. We have two parameters `theta1` and `theta2`, which we'll just start from the true values for the sake of demonstration purposes
```{r}
params = list( "theta1" = c( 0, 0 ), "theta2" = c( 0.1, 0.1 ) )
```

Now we'll define the functions `logLik` and `logPrior`. It should now become clear why the list names come in handy. The function `logLik` should take two parameters as input: `params` and `dataset`. These parameters will be lists with the same names as those you defined for `params` and `dataset` earlier. There is one difference though, the objects in the lists will have automatically been converted to `TensorFlow` objects for you. The `params` list will contain `TensorFlow` tensor variables; the `dataset` list will contain `TensorFlow` placeholders. The `logLik` function should take these lists as input and return the value of the log likelihood as a tensor at point `params` given data `dataset`. The function should do this using `TensorFlow` operations, as this allows the gradient to be automatically calculated; it also allows the wide range of distribution objects as well as matrix operations that `TensorFlow` provides to be taken advantage of. A tutorial of `TensorFlow` for `R` is beyond the scope of this article, for more details we refer the reader to the website of [TensorFlow for R](https://tensorflow.rstudio.com/). With this in place we can define the `logLik` function as follows
```{r}
logLik = function( params, dataset ) {
    # Declare Sigma as tensorflow constant (assumed known)
    Sigma = tf$constant( c(1, 1), dtype = tf$float32 )
    # Declare distribution of each component
    component1 = tf$contrib$distributions$MultivariateNormalDiag( params$theta1, Sigma )
    component2 = tf$contrib$distributions$MultivariateNormalDiag( params$theta2, Sigma )
    # Declare allocation probabilities of each component
    probs = tf$contrib$distributions$Categorical(c(0.5,0.5))
    # Declare full mixture distribution given components and allocation probabilities
    distn = tf$contrib$distributions$Mixture(probs, list(component1, component2))
    # Declare log likelihood
    logLik = tf$reduce_sum( distn$log_prob(dataset$X) )
    return( logLik )
}
```
So this function basically states that our likelihood is $\sum_{i=1}^N \log \left[ 0.7 \mathcal N( x_i | \theta_1, I_2 ) + 0.3 \mathcal N( x_i | \theta_2, I_2 ) \right]$, where $\mathcal N( x | \mu, \Sigma )$ is a Gaussian density at $x$ with mean $\mu$ and variance $\Sigma$. `R` does not play nicely with the tensorflow type system, so make sure you set all your constants in the `logLik` and `logPrior` functions to `tf$float32`, as we have done. Otherwise you will encounter an error.

Next we want to define our log prior, which we assume is uninformative $\log p( \theta ) = \mathcal N(\theta | 0,10I_2)$. Similar to the log likelihood definition, the log prior is defined as a function, but only with input `params`. In our case the definition is
```{r}
logPrior = function( params ) {
    # Declare hyperparameters mu0 and Sigma0 as tensorflow constants
    mu0 = tf$constant( c( 0, 0 ), dtype = tf$float32 )
    Sigma0 = tf$constant( c(10, 10), dtype = tf$float32 )
    # Declare prior distribution
    priorDistn = tf$contrib$distributions$MultivariateNormalDiag( mu0, Sigma0 )
    # Declare log prior density and return
    logPrior = priorDistn$log_prob( params$theta1 ) + priorDistn$log_prob( params$theta2 )
    return( logPrior )
}
```
Again make sure you set all your constants inside the function to `tf$float32`, as we have done.

Finally we set the tuning parameters for SGHMC and the minibatch size. Both the tuning parameters for SGHMC, momentum term `alpha` and stepsize term `eta`, are set for each parameter, and so are each lists with the same names as `params`. Following the advice in the [original paper](https://arxiv.org/pdf/1402.4102v2.pdf) we fix `alpha` at a small value like $0.1$ and then find a good value for `eta`. We set the trajectory `L` to be 3.
```{r}
eta = list( "theta1" = 5e-6, "theta2" = 5e-6 )
alpha = list( "theta1" = 0.01, "theta2" = 0.01 )
L = 3
minibatchSize = 200
```

Now we can run our SGHMC algorithm using the `sgmcmc` function `sghmc`, which returns a list of Markov chains for each parameter as output. Use the argument `verbose = FALSE` to hide the output of the function.
```{r message=FALSE}
chains = sghmc( logLik, logPrior, dataset, params, eta, alpha, L, minibatchSize, nIters = 10^4 )
```

Finally we'll plot the results after removing burn-in
```{r}
library(ggplot2)
# Remove burn in
burnIn = 10^3
chains = list( "theta1" = as.data.frame( chains$theta1[-c(1:burnIn),] ),
               "theta2" = as.data.frame( chains$theta2[-c(1:burnIn),] ) )
# Concatenate the two chains for the plot to get a picture of the whole distribution
plotData = rbind(chains$theta1, chains$theta2)
ggplot( plotData, aes( x = V1, y = V2 ) ) +
    stat_density2d( size = 1.5, alpha = 0.7 )
```

There are lots of other sgmcmc algorithms implemented that can be used by simply changing the tuning parameters slightly including `sgld` and `sgnht`; as well as their [control variate counterparts](https://arxiv.org/pdf/1706.05439.pdf) for improved efficiency.
